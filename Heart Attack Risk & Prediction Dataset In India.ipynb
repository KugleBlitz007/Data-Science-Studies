{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d2141e6-e3fd-4636-a761-0c6f48e3c235",
   "metadata": {},
   "source": [
    "Johann Rajosefa 300300054\n",
    "\n",
    "Kalala, Hilaire Junior 300289737\n",
    "\n",
    "Assignment 2 - CSI4142 - Group A-72\n",
    "\n",
    "Dataset 2: Heart Attack Risk & Prediction Dataset In India\n",
    "\n",
    "Comprehensive Cardiovascular Health Data Covering Risk Factors, Demographics.\n",
    "\n",
    "(No need to download the dataset since our code accesses it from our GitHub page)\n",
    "\n",
    "1) Introduction\n",
    "\n",
    "The purpose of analyzing this dataset is to evaluate different imputation techniques for handling missing data related to heart attack risk factors. Given that cardiovascular diseases (CVDs) are a major health concern in India, it is essential to ensure data completeness and accuracy for effective predictive modeling and analysis. This experiment will help determine which imputation methods perform best for handling missing values in key health-related attributes.\n",
    "\n",
    "2) Description of the Dataset\n",
    "\n",
    "Official website link : https://www.kaggle.com/datasets/ankushpanday2/heart-attack-risk-and-prediction-dataset-in-india/data\n",
    "\n",
    "Fast download link : https://github.com/KugleBlitz007/CSI4142/blob/main/heart_attack_prediction_india.csv\n",
    "\n",
    "Dataset Name : Heart Attack Risk & Prediction Dataset In India\n",
    "\n",
    "Author : Ankit\n",
    "\n",
    "Purpose : It can be used for predictive modeling, machine learning applications, epidemiological research, and policy analysis to improve early detection and intervention strategies for heart disease.\n",
    "\n",
    "Shape : This dataset contains 10000 rows (patient samples) and 26 columns\n",
    "\n",
    "Features :\n",
    "\n",
    "- Patient_ID - (Categorical): Unique identifier for each patient.  \n",
    "- State_Name - (Categorical): The state where the patient resides.  \n",
    "- Age - (Numerical): Patient’s age in years.  \n",
    "- Gender - (Categorical): Patient’s gender.\n",
    "- Diabetes - (Categorical) (Binary): Whether the patient has diabetes (Yes 1/No 0).  \n",
    "- Hypertension - (Categorical) (Binary): Whether the patient has high blood pressure (Yes 1/No 0).  \n",
    "- Obesity - (Categorical) (Binary): Whether the patient is classified as obese (Yes 1/No 0).  \n",
    "- Smoking - (Categorical) (Binary): Whether the patient is a smoker (Yes 1/No 0).  \n",
    "- Alcohol_Consumption - (Categorical) (Binary): If the patient consumes alcohol (Yes 1/ No 0).\n",
    "- Physical_Activity - (Categorical) (Binary): If the patient is physicaly active (Yes 1/ No 0). \n",
    "- Diet_Score - (Numerical): A score representing the healthiness of the patient's diet (0 to 10).  \n",
    "- Cholesterol_Level - (Numerical): Patient’s total cholesterol level in mg/dL.  \n",
    "- Triglyceride_Level - (Numerical): Triglyceride level in mg/dL.  \n",
    "- LDL_Level - (Numerical): Low-Density Lipoprotein (bad cholesterol) level in mg/dL.  \n",
    "- HDL_Level - (Numerical): High-Density Lipoprotein (good cholesterol) level in mg/dL.  \n",
    "- Systolic_BP - (Numerical): Systolic blood pressure (mmHg).  \n",
    "- Diastolic_BP - (Numerical): Diastolic blood pressure (mmHg).  \n",
    "- Air_Pollution_Exposure - (Categorical) (Binary): If the patietn is exposed to air polution (Yes 1/ No 0).  \n",
    "- Family_History - (Categorical) (Binary): Whether the patient has a family history of heart disease (Yes 1/No 0).  \n",
    "- Stress_Level - (Numerical): Self-reported stress level on a scale (1-10).  \n",
    "- Healthcare_Access - (Categorical) (Binary): If hte patient has access to healthcare (Yes 1/ No 0).\n",
    "- Heart_Attack_History - (Categorical) (Binary): Whether the patient has had a heart attack before (Yes 1/No 0).  \n",
    "- Emergency_Response_Time - (Numerical): Average emergency response time (We can assume its in minutes).  \n",
    "- Annual_Income - (Numerical): Patient’s income in local currency.  \n",
    "- Health_Insurance - (Categorical) (Binary): Whether the patient has health insurance (Yes 1/No 0).  \n",
    "- Heart_Attack_Risk - (Categorical) (Binary): Predicted risk of heart attack (Yes 1/No 0).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d88cc-2371-4249-9e5e-da6b032a4abc",
   "metadata": {},
   "source": [
    "We are now going to perform 3 tests where we use an imputation method and for each test, we are going to evaluate how accurate the method is at completing the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75b0fc2-4b5b-4fb3-b4e9-a399da182cf1",
   "metadata": {},
   "source": [
    "3) Imputation tests\n",
    "\n",
    "a) For the first test, we are going to chose the Cholesterol_Level attribute because it is a numerical value and the dataset shows a pretty uniform distribution. We are also going to simulate a MCAR removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "87d88d8c-e95a-476b-ae24-3831fabd4b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a Mean Absolute Error (MAE) of: 37.9370\n",
      "And we have a Mean Squared Error (MSE) of: 1899.5990\n",
      "\n",
      "The original values were:\n",
      "5097    184\n",
      "2753    171\n",
      "1375    171\n",
      "3537    281\n",
      "2687    278\n",
      "Name: Cholesterol_Level, dtype: int64\n",
      "\n",
      "The imputed values are:\n",
      "5097    225.0\n",
      "2753    225.0\n",
      "1375    225.0\n",
      "3537    225.0\n",
      "2687    225.0\n",
      "Name: Cholesterol_Level, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "\n",
    "# First we load the data from github\n",
    "# We saw this method from assignment 1\n",
    "GITHUB_CSV_URL = \"https://raw.githubusercontent.com/KugleBlitz007/CSI4142/refs/heads/main/heart_attack_prediction_india.csv\"\n",
    "response = requests.get(GITHUB_CSV_URL)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Secondly we select the Cholesterol_Level column and we are going to remove 20% of the data\n",
    "np.random.seed(300300054) \n",
    "missing_fraction = 0.2 \n",
    "cholesterol_col = \"Cholesterol_Level\"\n",
    "n_missing = int(missing_fraction * len(data))\n",
    "\n",
    "# Third we chose indices completly at random and remove them, but we retain the original values for \n",
    "# evaluation later\n",
    "missing_indices = np.random.choice(data.index, size=n_missing, replace=False)\n",
    "original_values = data.loc[missing_indices, cholesterol_col].copy()  \n",
    "data.loc[missing_indices, cholesterol_col] = np.nan  \n",
    "\n",
    "# Fourth we use a python function to find the median of the cholesterol column\n",
    "# And we replace every value of null to the median value\n",
    "median_value = data[cholesterol_col].median()\n",
    "data.loc[:, cholesterol_col] = data[cholesterol_col].fillna(median_value)\n",
    "\n",
    "# Finaly we evaluate the Median method using 2 evaluation formulas MAE and MSE \n",
    "imputed_values = data.loc[missing_indices, cholesterol_col]\n",
    "\n",
    "mae = mean_absolute_error(original_values, imputed_values)\n",
    "mse = mean_squared_error(original_values, imputed_values)\n",
    "\n",
    "# We display the result\n",
    "print(f\"We have a Mean Absolute Error (MAE) of: {mae:.4f}\")\n",
    "print(f\"And we have a Mean Squared Error (MSE) of: {mse:.4f}\")\n",
    "print(\"\")\n",
    "print(\"The original values were:\")\n",
    "print(original_values.head())\n",
    "print(\"\")\n",
    "print(\"The imputed values are:\")\n",
    "print(imputed_values.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1ea97-fd4a-4583-bc81-9f22d48ce2af",
   "metadata": {},
   "source": [
    "- Our first method using Default value imputation or Median imputation method showed a MAE of 37, considering the data values to be between 150 and 299, this is a significant deviation. This high deviation is also shown in the MSE because of the squarred values. This method removed the variability of the data in this column by replacing the missing data to a unique value (the median).\n",
    "\n",
    "- We can conclude that our approach in finding the missing values for this test using the Median imputation method was far from successfull."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e01f899-2dea-46bc-81c5-36ce9faa9054",
   "metadata": {},
   "source": [
    "b) For our second test, we are going to use the Systolic_BP or Systolic Blood pressure column since this <a href=\"https://pubmed.ncbi.nlm.nih.gov/18192832/\">article</a> stated that it is related to the Diastolic_BP or Diastolic Blood pressure. We are going to use a regression imputation by using the correlation between those columns after removing some values MAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6870ae67-3630-4d08-8eb6-973c0d14a35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Imputed Values:\n",
      "      Original     Imputed\n",
      "4722       163  134.334678\n",
      "8338       109  134.486804\n",
      "3019       172  134.499481\n",
      "5989       147  134.613575\n",
      "2084       131  134.689638\n",
      "...        ...         ...\n",
      "7226        92  134.689638\n",
      "8959       131  134.600898\n",
      "5376        98  134.689638\n",
      "6320       150  134.638929\n",
      "7529       153  134.448772\n",
      "\n",
      "[991 rows x 2 columns]\n",
      "\n",
      "We have a Mean Absolute Error (MAE) of: 21.9409\n",
      "And we have a Mean Squared Error (MSE) of: 642.2892\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# First we load the data from github\n",
    "# We saw this method from assignment 1\n",
    "GITHUB_CSV_URL = \"https://raw.githubusercontent.com/KugleBlitz007/CSI4142/refs/heads/main/heart_attack_prediction_india.csv\"\n",
    "response = requests.get(GITHUB_CSV_URL)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Secondly we select the Systolic_BP column and we are going to remove 20% of the data\n",
    "np.random.seed(300300054)  \n",
    "missing_fraction = 0.2\n",
    "systolic_col = \"Systolic_BP\"\n",
    "diastolic_col = \"Diastolic_BP\"\n",
    "\n",
    "# Third we chose indices at random and remove them, but we retain the original values for \n",
    "# evaluation later\n",
    "missing_indices = data[data[diastolic_col] > data[diastolic_col].median()].sample(frac=missing_fraction).index\n",
    "original_values = data.loc[missing_indices, systolic_col].copy() \n",
    "data.loc[missing_indices, systolic_col] = np.nan  \n",
    "\n",
    "# Then we train the regression model on the original values so it can predict the missing ones\n",
    "train_data = data.dropna(subset=[systolic_col, diastolic_col])\n",
    "X_train = train_data[[diastolic_col]]\n",
    "y_train = train_data[systolic_col]\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Now we predict the missing values\n",
    "X_missing = data.loc[missing_indices, [diastolic_col]]\n",
    "predicted_values = model.predict(X_missing)\n",
    "data.loc[missing_indices, systolic_col] = predicted_values\n",
    "\n",
    "# Finaly, we use MAE and MSE formulas to evaluate our method of retreiving missing data\n",
    "imputed_values = data.loc[missing_indices, systolic_col]\n",
    "\n",
    "mae = mean_absolute_error(original_values, imputed_values)\n",
    "mse = mean_squared_error(original_values, imputed_values)\n",
    "\n",
    "print(\"Original vs Imputed Values:\")\n",
    "print(pd.DataFrame({\"Original\": original_values, \"Imputed\": imputed_values}))\n",
    "print(f\"\\nWe have a Mean Absolute Error (MAE) of: {mae:.4f}\")\n",
    "print(f\"And we have a Mean Squared Error (MSE) of: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a1106a-bfab-49f0-bf07-a3096f181653",
   "metadata": {},
   "source": [
    "- Our result shows that there is a deviation of about 22 units from the imputed values and the original values, this is closer than the deviation of 37 units we got from the previous test; Additionaly the MSE result is also lower than the previous test. We can conclude that the regression Imputation method to retrieve missing data is more accurate than the median method; however, we may want a better and more accurate method, considering this is a health realated analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847d2cb1-2ea4-45d2-98d1-5589db753d6c",
   "metadata": {},
   "source": [
    "c) For our Final test, we are choosing the Diet_Score column and we are going to remove some data not randomnly (MNAR) because some patient may either be overestimating how healthy their food are or does not want to disclose that information. We are going to use the similarity-based imputation method to retrieve those missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d4e57965-ed9f-44fa-85bd-a4972ffe05c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vs Imputed Values:\n",
      "      Original  Imputed\n",
      "2167         0    5.080\n",
      "4600         3    4.976\n",
      "5542         6    4.928\n",
      "3548         1    5.109\n",
      "8391         0    4.898\n",
      "...        ...      ...\n",
      "3592         0    5.050\n",
      "2177         6    5.047\n",
      "864          0    4.934\n",
      "8229         7    5.115\n",
      "1731         5    4.940\n",
      "\n",
      "[607 rows x 2 columns]\n",
      "\n",
      "We have a Mean Absolute Error (MAE) of: 2.7177\n",
      "And we have a Mean Squared Error (MSE) of: 9.9790\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# First we load the data from github\n",
    "# We saw this method from assignment 1\n",
    "GITHUB_CSV_URL = \"https://raw.githubusercontent.com/KugleBlitz007/CSI4142/refs/heads/main/heart_attack_prediction_india.csv\"\n",
    "response = requests.get(GITHUB_CSV_URL)\n",
    "data = pd.read_csv(StringIO(response.text))\n",
    "\n",
    "# Secondly, we select the Diet_score column and we arbitrary choose to remove data whenever \n",
    "# the patient was obese.\n",
    "np.random.seed(300300054)  \n",
    "missing_fraction = 0.2  \n",
    "diet_col = \"Diet_Score\"\n",
    "obesity_col = \"Obesity\" \n",
    "\n",
    "# We manualy replace the diet values to NaN and we retaint the original values for evaluation\n",
    "missing_indices = data[data[obesity_col] == 1].sample(frac=missing_fraction).index\n",
    "original_values = data.loc[missing_indices, diet_col].copy()  \n",
    "data.loc[missing_indices, diet_col] = np.nan  \n",
    "\n",
    "# We use this function to build our similarity based imputation method\n",
    "imputer = KNNImputer(n_neighbors=1000, weights=\"uniform\")\n",
    "numeric_data = data.select_dtypes(include=[np.number])  \n",
    "imputed_array = imputer.fit_transform(numeric_data) \n",
    "data[numeric_data.columns] = imputed_array  # Restore imputed values back into DataFrame\n",
    "\n",
    "# Finaly we use the MAE and MSE formulas to evaluate our method\n",
    "imputed_values = data.loc[missing_indices, diet_col]\n",
    "\n",
    "mae = mean_absolute_error(original_values, imputed_values)\n",
    "mse = mean_squared_error(original_values, imputed_values)\n",
    "\n",
    "print(\"Original vs Imputed Values:\")\n",
    "print(pd.DataFrame({\"Original\": original_values, \"Imputed\": imputed_values}))\n",
    "print(f\"\\nWe have a Mean Absolute Error (MAE) of: {mae:.4f}\")\n",
    "print(f\"And we have a Mean Squared Error (MSE) of: {mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9148eb-5af5-4dcd-a712-8ae0e1008bc6",
   "metadata": {},
   "source": [
    "- We can conclude that this test out of all 3 is the closest to find the original values because it only has one or two units separating them in average; Additionaly the MSE is not out of control either. The KNN method of imputation is the most efficient for retrieving missing data, this is a crucial finding since this is a health related dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cc7969-aaa4-412e-ac17-614de53a083b",
   "metadata": {},
   "source": [
    "4) References :\n",
    "\n",
    "- ChatGPT 4o\n",
    "- https://www.geeksforgeeks.org\n",
    "- https://www.youtube.com/watch?v=R15LjD8aCzc\n",
    "- https://pubmed.ncbi.nlm.nih.gov/18192832/\n",
    "- https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3ccaab-9db1-4edc-8a90-2a3b2903bea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
